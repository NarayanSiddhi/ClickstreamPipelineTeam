app {
  version = "1.0"


  input {
    clickstreamPath = "C:\\ClickstreamProject\\src\\test\\scala\\data_in\\clickstream_log (1).csv"
    itemsetPath = "C:\\ClickstreamProject\\src\\test\\scala\\data_in\\item_data (1).csv"
  }

  output {
    joinedDatasetPath = "C:\\ClickstreamProject\\src\\test\\scala\\data_out\\click_stream_event_item"
    nullClickstream = "C:\\ClickstreamProject\\src\\test\\scala\\data_out\\nullClickstream"
    nullItemset = "C:\\ClickstreamProject\\src\\test\\scala\\data_out\\nullItemset"
    duplicateClickstream = "C:\\ClickstreamProject\\src\\test\\scala\\data_out\\duplicateClickstream"
    duplicateItemset = "C:\\ClickstreamProject\\src\\test\\scala\\data_out\\duplicateItemset"
    invalidItemPrice = "C:\\ClickstreamProject\\src\\test\\scala\\data_out\\invalidItemPrice"
    invalidEventTimestamp = "C:\\ClickstreamProject\\src\\test\\scala\\data_out\\invalidEventTimestamp"
  }

  spark {
    master = "local[*]"
    appName = "ClickstreamDataPipeline"
    logLevel = "ERROR"
    //spark.executor.memory="2g"
    //spark.default.parallelism=4
    //spark.sql.shuffle.partitions=10
    //spark.streaming.backpressure.enabled=true
    //spark.streaming.kafka.maxRatePerPartition=1000
  }

  jdbc {
    jdbcUrl = "jdbc:mysql://localhost:3306/clickstreamdata"
    jdbcUser = "root"
    jdbcPassword = "0702@Sidd"
  }

  thresholdValue{
    itemPriceLowerlimit=0.0
    itemPriceUpperlimit=1000.0
    eventTimestampLowerlimit="2020-01-01 00:00:00"
    eventTimestampUpperlimit="2023-12-31 23:59:59"
  }
}