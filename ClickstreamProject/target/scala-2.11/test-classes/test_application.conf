app{
  name="ClickstreamDataPipeline"
  version="1.0"
}

input{
  sample_path1="C:\\ClickstreamProject\\src\\test\\scala\\test_data_in\\Test_Sample_Clickstream.csv"
  sample_path2="C:\\ClickstreamProject\\src\\test\\scala\\test_data_in\\Test_Sample_ItemSet.csv"
}

output{
  sample_path="C:\\ClickstreamProject\\src\\test\\scala\\test_data_out\\click_stream_event_item"
  sampleNullClickstream = "C:\\ClickstreamProject\\src\\test\\scala\\test_data_out\\nullClickstream"
  sampleNullItemset = "C:\\ClickstreamProject\\src\\test\\scala\\test_data_out\\nullItemset"
  sampleDuplicateClickstream = "C:\\ClickstreamProject\\src\\test\\scala\\test_data_out\\duplicateClickstream"
  sampleDuplicateItemset = "C:\\ClickstreamProject\\src\\test\\scala\\test_data_out\\duplicateItemset"
}

spark{
  master="local[*]"
  appName=${app.name}
  logLevel="ERROR"
  //spark.executor.memory="2g"
  //spark.default.parallelism=4
  //spark.sql.shuffle.partitions=10
  //spark.streaming.backpressure.enabled=true
  //spark.streaming.kafka.maxRatePerPartition=1000
}